{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvHAWMW78AG9"
      },
      "source": [
        "# Train GPT-2 to Generate Tweets\n",
        "\n",
        "<font size=\"2\">*Adapted from [HuggingFace](https://colab.research.google.com/github/borisdayma/huggingtweets/blob/master/huggingtweets-demo.ipynb).*</font>\n",
        "\n",
        "Generating realistic text has become more and more efficient with models such as [GPT-2](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf). Those models are trained on very large datasets and require heavy computer resources (and time!).\n",
        "\n",
        "However, we can use Transfer Learning and a single GPU to quickly fine-tune a pre-trained model on a given task.\n",
        "\n",
        "We test if we can imitate the writing style of a Twitter user by only using some of his tweets. Twitter API let us download \"only\" the 3200 most recent tweets from any single user, which we then filter out (to remove retweets, short content, etc).\n",
        "\n",
        "Here is an example for Elon Musk's next breakthrough 😉\n",
        "\n",
        "![HuggingTweets Illustration](https://raw.githubusercontent.com/borisdayma/huggingtweets/master/img/example.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "id": "ZSCf6QyF8AG-"
      },
      "source": [
        "#####################\n",
        "# Check if we have  #\n",
        "# access to the GPU #\n",
        "#####################\n",
        "\n",
        "from IPython.display import clear_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tweepy\n",
        "!pip install torch transformers wandb -qqq\n",
        "!curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash\n",
        "!sudo apt-get install git-lfs\n",
        "\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "X0LsEeTc60WT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set up a Twitter Development Account\n",
        "\n",
        "In order to access Twitter data, we need to:\n",
        "\n",
        "* [Create a Twitter development account](https://developer.twitter.com/en/apply-for-access)\n",
        "* [Create a Twitter app](https://developer.twitter.com/en/apps)\n",
        "* Fet your consumer API keys: `API key` and `API Secret Key`\n",
        "\n",
        "The entire process only takes a few minutes."
      ],
      "metadata": {
        "id": "qR31g4Yf9yqX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#############################\n",
        "# Enter your credentials    #\n",
        "# (don't share with anyone) #\n",
        "#############################\n",
        "\n",
        "consumer_key    = 'CONSUMER_KEY'\n",
        "consumer_secret = 'CONSUMER_SECRET'"
      ],
      "metadata": {
        "id": "zecbERwP9x6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download @User Tweets\n",
        "\n",
        "We download latest tweets associated to a user account through [Tweepy](http://docs.tweepy.org/)."
      ],
      "metadata": {
        "id": "zVEnmOys-LcO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tweepy\n",
        "\n",
        "auth = tweepy.AppAuthHandler(consumer_key, consumer_secret)\n",
        "api  = tweepy.API(auth)"
      ],
      "metadata": {
        "id": "YuIA1NYr-JPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We grab all available tweets (limited to 3200 per API limitations) based on Twitter handle.\n",
        "\n",
        "**Note**: Protected users may only be requested when the authenticated user either \"*owns*\" the timeline or is an approved follower of the owner."
      ],
      "metadata": {
        "id": "XSQ44GIS-Vhn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "handle = 'elonmusk'"
      ],
      "metadata": {
        "id": "hn3jsTJQ-Re_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Adapted from https://gist.github.com/onmyeoin/62c72a7d61fc840b2689b2cf106f583c\"\"\"\n",
        "\n",
        "############################################\n",
        "# Initialize a list to hold all the tweepy #\n",
        "# Tweets & list with no retweets           #\n",
        "############################################\n",
        "\n",
        "user_tweets = []\n",
        "\n",
        "#################################################\n",
        "# make initial request for most recent tweets   #\n",
        "# with extended mode enabled to get full tweets #\n",
        "#################################################\n",
        "\n",
        "latest_tweets = api.user_timeline(screen_name=handle, tweet_mode='extended', count=200)\n",
        "\n",
        "if latest_tweets:\n",
        "\n",
        "    user_tweets.extend(latest_tweets)\n",
        "\n",
        "    ####################################################\n",
        "    # save the id of the oldest tweet decreased by one #\n",
        "    ####################################################\n",
        "\n",
        "    oldest = user_tweets[-1].id - 1\n",
        "\n",
        "    while True:\n",
        "\n",
        "        #####################################\n",
        "        # all subsequent requests use the   #\n",
        "        # ax_id param to prevent duplicates #\n",
        "        #####################################\n",
        "\n",
        "        extra_tweets = api.user_timeline(screen_name=handle, tweet_mode='extended', count=200, max_id=oldest)\n",
        "\n",
        "        ############################################\n",
        "        # stop if no more tweets (try a few        #\n",
        "        # times as they sometimes eventually come) #\n",
        "        ############################################\n",
        "\n",
        "        if not extra_tweets: break\n",
        "\n",
        "        user_tweets.extend(extra_tweets)\n",
        "\n",
        "        oldest = extra_tweets[-1].id - 1\n",
        "\n",
        "        print(f'Downloaded {len(extra_tweets)} tweets so far.')\n",
        "\n",
        "n_tweets = len(user_tweets)\n",
        "\n",
        "print(f'\\nGrabbed {n_tweets} tweets from @{handle}.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpzjZtPN-cXX",
        "outputId": "1daaee5c-ca35-4877-e207-27cc6f042dae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded 200 tweets so far.\n",
            "Downloaded 199 tweets so far.\n",
            "Downloaded 199 tweets so far.\n",
            "Downloaded 200 tweets so far.\n",
            "Downloaded 200 tweets so far.\n",
            "Downloaded 200 tweets so far.\n",
            "Downloaded 199 tweets so far.\n",
            "Downloaded 198 tweets so far.\n",
            "Downloaded 200 tweets so far.\n",
            "Downloaded 200 tweets so far.\n",
            "Downloaded 200 tweets so far.\n",
            "Downloaded 199 tweets so far.\n",
            "Downloaded 200 tweets so far.\n",
            "Downloaded 199 tweets so far.\n",
            "Downloaded 200 tweets so far.\n",
            "\n",
            "Grabbed 3193 tweets from @elonmusk.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the text from tweets and remove `RT`"
      ],
      "metadata": {
        "id": "mXalSnmj_Z7O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_list = [tweet.full_text for tweet in user_tweets if not hasattr(tweet, 'retweeted_status')]\n",
        "\n",
        "print(f'Found {n_tweets} tweets, including {n_tweets - len(tweet_list)} RT, keeping {len(tweet_list)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11Wx2nx7_UWv",
        "outputId": "b429997f-a245-437f-f0db-36c1edc1dda9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3193 tweets, including 175 RT, keeping 3018\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a dataset from downloaded tweets\n",
        "\n",
        "We remove:\n",
        "* Retweets (since it's not in the wording style of target author)\n",
        "* Tweets with no interesting content (limited to URL's, User Mentionss, `thank you`…)\n",
        "\n",
        "We clean up remaining tweets:\n",
        "* We remove URL's\n",
        "* We correct special characters"
      ],
      "metadata": {
        "id": "cY_PXTft_k2-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random, re, torch"
      ],
      "metadata": {
        "id": "NLCJOYfU_i4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Total number of tweets: {len(tweet_list)} / {len(user_tweets)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lw8q5I4OERfw",
        "outputId": "636cd1d4-d803-442d-864d-c1185e2b57a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of tweets: 3018 / 3193\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def fix_text(text):\n",
        "    text = text.replace('&amp;', '&')\n",
        "    text = text.replace('&lt;', '<')\n",
        "    text = text.replace('&gt;', '>')\n",
        "    return text"
      ],
      "metadata": {
        "id": "pVz1jhtIEUco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_tweet(tweet, allow_new_lines = False):\n",
        "\n",
        "    bad_start = ['http:', 'https:']\n",
        "\n",
        "    for w in bad_start:\n",
        "        tweet = re.sub(f\" {w}\\\\S+\", \"\", tweet)      # removes white space before url\n",
        "        tweet = re.sub(f\"{w}\\\\S+ \", \"\", tweet)      # in case a tweet starts with a url\n",
        "        tweet = re.sub(f\"\\n{w}\\\\S+ \", \"\", tweet)    # in case the url is on a new line\n",
        "        tweet = re.sub(f\"\\n{w}\\\\S+\", \"\", tweet)     # in case the url is alone on a new line\n",
        "        tweet = re.sub(f\"{w}\\\\S+\", \"\", tweet)       # any other case?\n",
        "\n",
        "    tweet = re.sub(' +', ' ', tweet)                # replace multiple spaces with one space (makes the previous work worthless?)\n",
        "\n",
        "    if not allow_new_lines: tweet = ' '.join(tweet.split())\n",
        "\n",
        "    return tweet.strip()"
      ],
      "metadata": {
        "id": "pFq5EjGXEatY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def boring_tweet(tweet):\n",
        "    \"\"\"Check if this is a boring tweet\"\"\"\n",
        "\n",
        "    boring_stuff     = ['http', '@', '#']\n",
        "    not_boring_words = len([None for w in tweet.split() if all(bs not in w.lower() for bs in boring_stuff)])\n",
        "\n",
        "    return not_boring_words < 3"
      ],
      "metadata": {
        "id": "O3lC-4DzEd44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "curated_tweets     = [fix_text(tweet) for tweet in tweet_list]\n",
        "clean_tweets       = [clean_tweet(tweet) for tweet in curated_tweets]\n",
        "informative_tweets = [tweet for tweet in clean_tweets if not boring_tweet(tweet)]"
      ],
      "metadata": {
        "id": "6WNodCQBEioI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Total number of tweets: {len(informative_tweets)} / {len(tweet_list)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQ91tjimEua4",
        "outputId": "778464dd-791a-4163-e91a-8454e65b5cf8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of tweets: 1922 / 3018\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###################################\n",
        "# Create a file based on multiple #\n",
        "# epochs with tweets mixed up     #\n",
        "###################################\n",
        "\n",
        "total_text = '<|endoftext|>' + '<|endoftext|>'.join(informative_tweets) + '<|endoftext|>'"
      ],
      "metadata": {
        "id": "NsxRIf-ygIkv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training\n",
        "\n",
        "For GPT-2 fine-tuning, we are only using a `124M` model here but gpt-2 has the option to use `355M` or `774M` model."
      ],
      "metadata": {
        "id": "bZl1VH5mKRCt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "allow_new_lines = False\n",
        "learning_rate   = 1.372e-4\n",
        "epochs          = 4"
      ],
      "metadata": {
        "id": "gChdM7CngXjf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TextDataset,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    get_cosine_schedule_with_warmup\n",
        ")"
      ],
      "metadata": {
        "id": "cuxvfoETFv1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving the `informative tweets` corpus as text file"
      ],
      "metadata": {
        "id": "d4AIhH73K49f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file = open('tweet.txt', 'w')\n",
        "\n",
        "file.write(total_text)\n",
        "file.close()"
      ],
      "metadata": {
        "id": "uZ20DSpUKiVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
        "\n",
        "model     = AutoModelForCausalLM.from_pretrained('gpt2', cache_dir=pathlib.Path('cache').resolve())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_-l4glxK3Al",
        "outputId": "add6b13b-fbc6-431c-c8cd-5cb4966dbbbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/config.json\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/vocab.json\n",
            "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/merges.txt\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/tokenizer.json\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/config.json\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /content/cache/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/config.json\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /content/cache/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/pytorch_model.bin\n",
            "Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /content/cache/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"_from_model_config\": true,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "block_size    = tokenizer.model_max_length\n",
        "train_dataset = TextDataset(tokenizer=tokenizer, file_path=f\"tweet.txt\", block_size=block_size, overwrite_cache=True)\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "seed = random.randint(0,2**32-1)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir                  = f\"output/{handle}\",\n",
        "    overwrite_output_dir        = True,\n",
        "    do_train                    = True,\n",
        "    num_train_epochs            = 5,\n",
        "    per_device_train_batch_size = 1,\n",
        "    prediction_loss_only        = True,\n",
        "    logging_steps               = 5,\n",
        "    save_steps                  = 0,\n",
        "    seed                        = seed,\n",
        "    learning_rate               = learning_rate\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wPgKi27DipsF",
        "outputId": "f17b6437-0216-4969-8a12-14d95f1373aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Creating features from dataset file at \n",
            "Saving features into cached file cached_lm_GPT2TokenizerFast_1024_tweet.txt [took 0.001 s]\n",
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "combined_dict = {**model.config.to_dict(), **training_args.to_sanitized_dict()}"
      ],
      "metadata": {
        "id": "2CHxsx3BkbNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "  model         = model,\n",
        "  tokenizer     = tokenizer,\n",
        "  args          = training_args,\n",
        "  data_collator = data_collator,\n",
        "  train_dataset = train_dataset\n",
        ")"
      ],
      "metadata": {
        "id": "R3RogRnYkefk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = trainer.get_train_dataloader()\n",
        "num_train_steps  = len(train_dataloader)\n",
        "\n",
        "trainer.create_optimizer_and_scheduler(num_train_steps)\n",
        "\n",
        "trainer.lr_scheduler = get_cosine_schedule_with_warmup(\n",
        "    trainer.optimizer,\n",
        "    num_warmup_steps   = 0,\n",
        "    num_training_steps = num_train_steps\n",
        ")"
      ],
      "metadata": {
        "id": "-8f9YHv2knUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "UiEzGFPYkuXk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c4b17dbd-7b49-4c5f-b66b-205aab8e8f3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 42\n",
            "  Num Epochs = 5\n",
            "  Instantaneous batch size per device = 1\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 210\n",
            "  Number of trainable parameters = 124439808\n",
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='210' max='210' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [210/210 01:23, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>4.628200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>4.409500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>4.254700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>4.196800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>4.158600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>4.008300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>4.107800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>3.955200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>3.713700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>3.925500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>3.853500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>3.748600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>3.794300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>3.614200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>3.636100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>3.614500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85</td>\n",
              "      <td>3.448000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>3.480500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>95</td>\n",
              "      <td>3.265800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>3.199900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>105</td>\n",
              "      <td>3.318100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>3.170000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>115</td>\n",
              "      <td>3.203200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>3.213600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>125</td>\n",
              "      <td>3.236800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>3.201300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>135</td>\n",
              "      <td>3.197700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>2.890900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>145</td>\n",
              "      <td>3.032100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>2.821100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>155</td>\n",
              "      <td>3.098400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>3.040500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>165</td>\n",
              "      <td>3.012500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>3.014300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>175</td>\n",
              "      <td>2.626300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>2.852300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>185</td>\n",
              "      <td>3.015200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>2.958500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>195</td>\n",
              "      <td>2.796200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>2.631900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>205</td>\n",
              "      <td>2.865900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>2.615200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=210, training_loss=3.4006070091610865, metrics={'train_runtime': 84.4011, 'train_samples_per_second': 2.488, 'train_steps_per_second': 2.488, 'total_flos': 109742653440000.0, 'train_loss': 3.4006070091610865, 'epoch': 5.0})"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.model.config.task_specific_params['text-generation'] = {\n",
        "  'do_sample':   True,\n",
        "  'min_length':  10,\n",
        "  'max_length':  160,\n",
        "  'temperature': 1.0,\n",
        "  'top_p':       0.95,\n",
        "  'prefix':      '<|endoftext|>'\n",
        "}"
      ],
      "metadata": {
        "id": "ZgzbhCYXk3g1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPT-2 Tweet Generation\n",
        "\n",
        "Let’s take a look at some of the tweets that our fine-tuned model can generate."
      ],
      "metadata": {
        "id": "UJc0glxdLULs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start = 'My dream is'"
      ],
      "metadata": {
        "id": "dmgXRJZznotz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_with_bos = '<|endoftext|>' + start\n",
        "\n",
        "encoded_prompt = trainer.tokenizer(start_with_bos, add_special_tokens=False, return_tensors=\"pt\").input_ids\n",
        "encoded_prompt = encoded_prompt.to(trainer.model.device)"
      ],
      "metadata": {
        "id": "k4R2VeCqUjPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* `temperature`: governs the randomness and thus the creativity of the responses. A temperature of 0 means roughly that the model will always select the highest probability word.\n",
        "A higher temperature means that the model might select a word with slightly lower probability, leading to more variation, randomness and creativity.\n",
        "A very high temperature therefore increases the risk of “hallucination”, meaning that the AI starts selecting words that will make no sense or be offtopic.\n",
        "\n",
        "* `num_beams`: refers to beam search, which is used for text generation. It returns the n most probable next words, rather than greedy search which returns the most probable next word. Instead of greedily choosing the most likely next step as the sequence is constructed, the beam search expands all possible next steps and keeps the k most likely, where k is a user-specified parameter and controls the number of beams or parallel searches through the sequence of probabilities."
      ],
      "metadata": {
        "id": "rszhxin-IEYQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_sequences = trainer.model.generate(\n",
        "  input_ids            = encoded_prompt,\n",
        "  max_length           = 160,              # The maximum length the generated tokens can have.\n",
        "  min_length           = 10,               # The minimum length of the sequence to be generated.\n",
        "  temperature          = 1.0,              # The value used to modulate the next token probabilities.\n",
        "\n",
        "  top_p                = 0.95,             # If set to float < 1, only the smallest set of most probable tokens with probabilities that add up to top_p or higher are kept for generation.\n",
        "  do_sample            = True,             # Whether or not to use sampling ; use greedy decoding otherwise.\n",
        "  num_return_sequences = 10\n",
        ")"
      ],
      "metadata": {
        "id": "5PGBgxUhWZ9u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bc24a62-b52b-4dc2-8ecc-958ddae9a94b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generated_sequences = []\n",
        "predictions         = []\n",
        "\n",
        "for generated_sequence_idx, generated_sequence in enumerate(output_sequences):\n",
        "\n",
        "    generated_sequence = generated_sequence.tolist()\n",
        "\n",
        "    text = trainer.tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True, skip_special_tokens=True)\n",
        "\n",
        "    if not allow_new_lines:\n",
        "        limit = text.find('\\n')\n",
        "        text  = text[: limit if limit != -1 else None]\n",
        "\n",
        "    generated_sequences.append(text.strip())\n",
        "\n",
        "for i, g in enumerate(generated_sequences): predictions.append([start, g])"
      ],
      "metadata": {
        "id": "ByoixuEIn5b0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for s in generated_sequences: print(s)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEff_FkQoD7E",
        "outputId": "ad531e1e-a560-40e7-a277-bbc63ca8b48e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My dream is to be able to upload videos & music for my friends & followers simultaneously in both my home & office. We will take that next step.\n",
            "My dream is to make a game engine that is more beautiful for every person, not just for some. Will do so by setting the minimum goal for a game size of 1Ghz for the rest of Earth.\n",
            "My dream is to bring people that love the game and understand the value of the product.\n",
            "My dream is to become the first Russian citizen to hold an honorary Federal Reserve Chair. As it happens, it is the least he can do:\n",
            "My dream is to play that game in the dark. That is the goal. We will add a second option later this week.\n",
            "My dream is that my children will play an epic video game on the back of my home computer\n",
            "My dream is that I will try my best to do my part to make The New York Times look better!\n",
            "My dream is for the future of Twitter!\n",
            "My dream is that the future, but at present the facts don't match.\n",
            "My dream is a world where robots are the go-to-read of the people on the phone. We need to save our civilization from being corrupted.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RgRZUERwNNNh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}