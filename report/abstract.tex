Developing a generative AI-powered chatbot tailored for Fog/Edge Emulation
deployments represents an exciting convergence of cutting-edge technology. This
endeavor draws inspiration from the capabilities of advanced Generative AI
models such as ChatGPT, which excel in comprehending and generating human-like
text. The primary objective here is to harness the potential of these models to
streamline and enrich the process of configuring Fog/Edge Emulation systems. To
achieve this, we design a user-friendly API that allows clients to
submit their requirements for a Fog Computing infrastructure. The system will
then extract relevant information from these inquiries and augment them with
the corresponding context. This augmentation will be achieved through the
implementation of prompt engineering techniques and in-context learning.
Subsequently, the system will transmit these enhanced queries to a powerful
large language model (LLM), such as the ChatGPT API, and relay the LLM's
responses back to the client. In this project we assume Fogify as the
underlying
emulation engine, and we create prompt engineering, using the  modeling
abstractions provided by Fogify's dedicated documentation page.
