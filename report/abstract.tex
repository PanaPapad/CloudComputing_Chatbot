Developing a generative AI-powered chatbot tailored for Fog/Edge Emulation
deployments represents an exciting convergence of cutting-edge technology. This
endeavor draws inspiration from the capabilities of advanced Generative AI
models such as ChatGPT, which excel in comprehending and generating human-like
text. The primary objective here is to harness the potential of these models to
streamline and enrich the process of configuring Fog/Edge Emulation systems. To
achieve this, students will design a user-friendly API that allows clients to
submit their requirements for a Fog Computing infrastructure. The system will
then extract relevant information from these inquiries and augment them with
the corresponding context. This augmentation will be achieved through the
implementation of prompt engineering techniques and in-context learning.
Subsequently, the system will transmit these enhanced queries to a powerful
large language model (LLM), such as the ChatGPT API, and relay the LLM's
responses back to the client. Students will consider Fogify as the underlying
emulation engine, and to facilitate their prompt engineering, they will utilize
the modeling abstractions provided by Fogify's dedicated documentation page.